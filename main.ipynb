{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unveiling Network Threats: Anomaly Detection in Intrusion Detection Systems\n",
    "\n",
    "**Author:** MÃ¡rio Antunes (mario.antunes@ua.pt)\n",
    "\n",
    "**Date:** 24/11/2023\n",
    "\n",
    "This notebook explores unsupervised methods to flag malicious packet flows.\n",
    "\n",
    "The dataset was gathered by the [GCS](https://www.ua.pt/pt/ciberseguranca/sobre-gcs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip\n",
    "import requests\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import geopy.distance\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Enhance the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pl.read_csv('https://cloud.hrun.duckdns.org/s/ZbwzSwEPk6pEds2/download/user-asn-sorted.csv', null_values=['-'])\n",
    "\n",
    "f=gzip.open('user-asn-sorted.csv.gz','rb')\n",
    "file_content=f.read()\n",
    "df = pl.read_csv(file_content, null_values=['-'])\n",
    "\n",
    "df = df.with_columns(pl.col('timestamp').str.to_datetime('%Y-%m-%dT%H:%M:%S'))\n",
    "df = df.drop('malware_type')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = df[['signature']].unique()['signature'].to_list()\n",
    "dict_anomalies = {}\n",
    "for k in signatures:\n",
    "    if k is not None:\n",
    "        dict_anomalies[k] = 1\n",
    "\n",
    "df = df.with_columns(pl.col('signature').map_dict(dict_anomalies, default=0).alias('anomaly'))\n",
    "df = df.drop('signature')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = requests.get('https://iptoasn.com/data/ip2asn-v4-u32.tsv.gz', headers={'User-agent': 'Mozilla/5.0'})\n",
    "#ASNs = pl.read_csv(r.content, separator='\\t', has_header=False, new_columns=['start.ip','stop.ip','asn','country.code2','entity'])\n",
    "\n",
    "f=gzip.open('ip2asn-v4-u32.tsv.gz','rb')\n",
    "file_content=f.read()\n",
    "ASNs = pl.read_csv(file_content, separator='\\t', has_header=False, new_columns=['start.ip','stop.ip','asn','country.code2','entity'])\n",
    "\n",
    "ASNs = ASNs.filter(pl.col('asn') != 0)\n",
    "ASNs = ASNs[['asn','country.code2','entity']].unique()\n",
    "ASNs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(ASNs, on='asn')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries = pl.read_csv('https://cloud.hrun.duckdns.org/s/qDCMSH5HbHKGszE/download/countries_codes_and_coordinates.csv', null_values=['-'])\n",
    "countries = pl.read_csv('countries_codes_and_coordinates.csv', null_values=['-'])\n",
    "countries = countries.drop(['country.code3','country.code.num'])\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_list = countries.rows()\n",
    "number_countries = len(countries_list)\n",
    "countries_distance = np.zeros((number_countries, number_countries))\n",
    "\n",
    "for i in range(number_countries-1):\n",
    "    for j in range(i, number_countries):\n",
    "        coords_1 = (countries_list[i][2], countries_list[i][3])\n",
    "        coords_2 = (countries_list[j][2], countries_list[j][3])\n",
    "        dist = geopy.distance.geodesic(coords_1, coords_2).km\n",
    "        countries_distance[i][j] = dist\n",
    "        countries_distance[j][i] = dist\n",
    "\n",
    "largest_distance = np.max(countries_distance)\n",
    "\n",
    "np.fill_diagonal(countries_distance, largest_distance)\n",
    "\n",
    "countries_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_rank = [0] * number_countries\n",
    "\n",
    "COUNTRY_IDX = 0\n",
    "for i in range(number_countries):\n",
    "    if countries_list[i][1] == 'PT':\n",
    "        COUNTRY_IDX = i\n",
    "        break\n",
    "COUNTRY_IDX\n",
    "\n",
    "rank = 0\n",
    "processed_countries = [COUNTRY_IDX]\n",
    "countries_rank[COUNTRY_IDX] = rank\n",
    "rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(processed_countries) < number_countries:\n",
    "    distances = np.zeros(number_countries)\n",
    "    for cidx in processed_countries:\n",
    "        distances += countries_distance[cidx]\n",
    "    COUNTRY_IDX = np.argmin(distances)\n",
    "\n",
    "    while COUNTRY_IDX in processed_countries:\n",
    "        distances[COUNTRY_IDX] = float('inf')\n",
    "        COUNTRY_IDX = np.argmin(distances)\n",
    "\n",
    "    processed_countries.append(COUNTRY_IDX)\n",
    "    countries_rank[COUNTRY_IDX] = rank\n",
    "    rank += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_contries = {}\n",
    "\n",
    "for i in range(number_countries):\n",
    "    map_contries[countries_list[i][0]] = countries_rank[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dst_ports = {0:0, 1:0, 5:0,\n",
    "7:10, 20:20, 21:20, 22:20, 23:20,\n",
    "25:40, 37:10, 43:10, 53:10, 80:30,\n",
    "81:50, 82:20, 83:0, 88: 20, 110:40,\n",
    "119:40, 123:10, 135:10, 137:10, 139:10,\n",
    "143:40, 161:10, 389:10, 443:30, 444:30, 445:20,\n",
    "465:30, 500:10, 514:20, 585:40, 587:40, 843:10,\n",
    "993:40, 995:40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(countries, on='country.code2')\n",
    "df = df.rename({'country.code2': 'country_code'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ['0028e812', 'cb8f966a', '6b6e3de0', 'f80b9c1b', '769d71e8', 'cd46ec92', '03aec668']\n",
    "\n",
    "connections = []\n",
    "\n",
    "for u in users:\n",
    "    single_user = df.filter(pl.col('user') == u)\n",
    "    connections.append(single_user.rows(named=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in connections:\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.stock_img()\n",
    "\n",
    "    for i in range(len(c)-1):\n",
    "        start = c[i]\n",
    "        stop = c[i+1]\n",
    "        plt.plot([start['lon'], stop['lon']], [start['lat'], \n",
    "        stop['lat']], color='blue', linewidth=1, marker='o', \n",
    "        transform=ccrs.Geodetic())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classifier (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[['user', 'dst_port', 'country', 'anomaly']]\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Specific Encoding\n",
    "\n",
    "The communications ports were grouped together into classes (for known ports below 1024) and adjusted accordingly.\n",
    "Regarding the countries (that are aligned with ASN entity), they are ranked together based on proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_user = df.select(pl.col('user').unique(maintain_order=True)).with_row_count(name='user_enc')\n",
    "df_clean = df_clean.join(df_unique_user, on='user')\n",
    "df_clean = df_clean.drop('user')\n",
    "df_clean = df_clean.rename({'user_enc': 'user'})\n",
    "df_clean = df_clean.with_columns(pl.col('dst_port').map_dict(map_dst_ports, default=100).alias('dst_port'))\n",
    "df_clean = df_clean.with_columns(pl.col('country').map_dict(map_contries, default=1000).alias('country'))\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious = df_clean.filter(pl.col('anomaly') == 1)\n",
    "df_benign    = df_clean.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "df_count = pl.DataFrame({'anomaly': ['Benign', 'Malicious'], 'count': [df_benign.shape[0], df_malicious.shape[0]]})\n",
    "sns.barplot(x='anomaly', y='count', hue='anomaly', data=df_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_clean['anomaly']\n",
    "X = df_clean.drop(['anomaly'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious = df_clean.filter(pl.col('anomaly') == 1)\n",
    "df_benign    = df_clean.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "df_benign_downsample = resample(df_benign, replace=True, \n",
    "n_samples=len(df_malicious), random_state=42)\n",
    "\n",
    "df_downsampled = pl.concat([df_malicious, df_benign_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled_malicious = df_downsampled.filter(pl.col('anomaly') == 1)\n",
    "df_downsampled_benign    = df_downsampled.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "df_count = pl.DataFrame({'anomaly': ['Benign', 'Malicious'], 'count': [df_downsampled_benign.shape[0], df_downsampled_malicious.shape[0]]})\n",
    "sns.barplot(x='anomaly', y='count', hue='anomaly', data=df_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_downsampled['anomaly']\n",
    "X = df_downsampled.drop(['anomaly'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packet Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_shift(df, columns, windows):\n",
    "    for i in range(1, windows):\n",
    "        df = df.with_columns((pl.col(columns).shift(i)).name.prefix(f'prev_{i}_'))\n",
    "    return df.drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "df_users = df_clean.partition_by(by='user')\n",
    "for i in range(len(df_users)):\n",
    "    df_aux = df_users[i]\n",
    "    df_users[i] = dataframe_shift(df_aux, columns=['dst_port', 'country'], windows=window)\n",
    "df_clean_roll = pl.concat(df_users)\n",
    "df_clean_roll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roll_malicious = df_clean_roll.filter(pl.col('anomaly') == 1)\n",
    "df_roll_benign    = df_clean_roll.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "df_count = pl.DataFrame({'anomaly': ['Benign', 'Malicious'], 'count': [df_roll_benign.shape[0], df_roll_malicious.shape[0]]})\n",
    "sns.barplot(x='anomaly', y='count', hue='anomaly', data=df_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_clean['anomaly']\n",
    "X = df_clean.drop(['anomaly'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious = df_clean_roll.filter(pl.col('anomaly') == 1)\n",
    "df_benign    = df_clean_roll.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "df_benign_downsample = resample(df_benign, replace=True, \n",
    "n_samples=len(df_malicious), random_state=42)\n",
    "\n",
    "df_downsampled = pl.concat([df_malicious, df_benign_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roll_downsampled_malicious = df_downsampled.filter(pl.col('anomaly') == 1)\n",
    "df_roll_downsampled_benign    = df_downsampled.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "df_count = pl.DataFrame({'anomaly': ['Benign', 'Malicious'], 'count': [df_roll_downsampled_benign.shape[0], df_roll_downsampled_malicious.shape[0]]})\n",
    "sns.barplot(x='anomaly', y='count', hue='anomaly', data=df_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_downsampled['anomaly']\n",
    "X = df_downsampled.drop(['anomaly'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_roll_malicious = df_clean_roll.filter(pl.col('anomaly') == 1)\n",
    "df_clean_roll_benign    = df_clean_roll.filter(pl.col('anomaly') == 0)\n",
    "\n",
    "Y = df_clean_roll_benign['anomaly']\n",
    "X = df_clean_roll_benign.drop(['anomaly'])\n",
    "\n",
    "k=3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=df_clean_roll_malicious.shape[0]*k, random_state=42)\n",
    "\n",
    "# Append the anomalies to the test\n",
    "y_test = pl.concat([y_test, df_clean_roll_malicious['anomaly']])\n",
    "X_test = pl.concat([X_test, df_clean_roll_malicious.drop(['anomaly'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation = train_test_split(X_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = np.clip(y_pred*-1, 0, 1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "\n",
    "for k in range(2, 22):\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    sse[k] = kmeans.inertia_\n",
    "\n",
    "plt.title('Elbow plot for K selection')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, n_init='auto', random_state=42)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = kmeans.transform(X_test)\n",
    "cluster_indices = kmeans.predict(X_test)\n",
    "mask = (cluster_indices[:, None] == np.arange(distances.shape[1]))\n",
    "distances[~mask] = 0\n",
    "\n",
    "dictionary = {i: column for i, column in enumerate(zip(*distances))}\n",
    "\n",
    "mean_distances = [sum(values) / len(values) for key, values in dictionary.items()]\n",
    "thresholds = np.array(mean_distances)\n",
    "\n",
    "print(f'thresholds: {thresholds}')\n",
    "k=3.0\n",
    "outliers = np.any(np.array(distances) > k*thresholds, axis=1)\n",
    "y_pred = [1 if v else 0 for v in outliers]\n",
    "print(f'{outliers}')\n",
    "print(f'{y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions // hyperparameters \n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 6\n",
    "\n",
    "encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, kernel_initializer='glorot_uniform', activation='elu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(latent_dim, kernel_initializer='glorot_uniform', activation='elu'),\n",
    "])\n",
    "\n",
    "decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, kernel_initializer='glorot_uniform', activation='elu', input_shape=(latent_dim,)),\n",
    "    tf.keras.layers.Dense(input_dim, kernel_initializer='glorot_uniform', activation='linear')\n",
    "])\n",
    "\n",
    "autoencoder = tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=[coeff_determination])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 4096\n",
    "history = autoencoder.fit(X_train, X_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "validation_data=(X_validation, X_validation), verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['coeff_determination'])\n",
    "plt.plot(history.history['val_coeff_determination'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious = mse[y_test==1]\n",
    "benign    = mse[y_test==0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.hist(malicious, bins=30, density=True, label=\"malicious\", alpha=.6, color=\"red\")\n",
    "ax.hist(benign, bins=30, density=True, label=\"benign\", alpha=.6, color=\"green\")\n",
    "\n",
    "plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pl.DataFrame({'mse': mse,'label': y_test})\n",
    "ax = sns.violinplot(x='label', y='mse', cut = 0, data=df_scores)\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['Benign', 'Malicious'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the class grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_scatter(features, labels):\n",
    "    # t-SNE dimensionality reduction\n",
    "    features_embedded = TSNE(n_components=2, random_state=42).fit_transform(features)\n",
    "    \n",
    "    # initialising the plot\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "    # plotting data\n",
    "    ax.scatter(*zip(*features_embedded[np.where(labels==1)]), marker='o',color='r', s=2,alpha=0.7,label='Malicious')\n",
    "    ax.scatter(*zip(*features_embedded[np.where(labels==0)]), marker='o',color='g',s=2, alpha=0.3,label='Benign')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_scatter(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Latent = encoder.predict(X_test)\n",
    "tsne_scatter(X_Latent, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier identification and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad_score(points):\n",
    "    m = np.median(points)\n",
    "    ad = np.abs(points - m)\n",
    "    mad = np.median(ad)\n",
    "    return 0.6745 * ad / mad\n",
    "\n",
    "THRESHOLD = 3.5\n",
    "z_scores = mad_score(mse)\n",
    "outliers = z_scores > THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, outliers, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, outliers)\n",
    "sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
